For the uncertainty in the mean:
https://www.physics.upenn.edu/sites/default/files/Managing%20Errors%20and%20Uncertainty.pdf

\section{Statistics definitions}\label{app:statistics_definitions}
This appendix, introduces the basic terminology of statistical analysis and gives the definitions that are used in this thesis. The definitions follow the book by R. J. Barlow~\cite{lvp.b313005720130101} where one can find a more detailed description.

\subsection{Averages}
\normalsize{\textbf{Arithmetic mean}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the arithmetic mean or just mean of the value of $x$ is:

\begin{equation}\label{eq:mean_def}
    \langle x \rangle = \frac{1}{N} \sum_{i=1}^{N} x_i.
\end{equation}
% compatible with numpy.mean --> /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb


\normalsize{\textbf{Root mean square}}\\
An alternative to the arithmetic mean is the root mean square (rms), which for the set of $N$ numbers is defined as:
\begin{equation}\label{eq:rms_def}
    x^{rms} = \sqrt{\frac{x_1^2+x_2^2+x_3^2+...+x_N^2}{N}}.
\end{equation}

In other words it is the square root of the arithmetic mean of the squares of the data set values. It is common in physics and in sciences in general to used the rms value instead of the arithmetic mean when it comes to distributions or when the random variables have both positive and negative values.

\subsection{Measuring the spread}
\normalsize{\textbf{Variance}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the variance of $x$  expresses how much it can vary from the mean value, $\langle x \rangle$. The variance, $\mathrm{Var}(x)$, is defined as:
\begin{equation}\label{eq:var_def}
    \mathrm{Var}(x) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2.
\end{equation}
% compatible with numpy.var, tested at: /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb

\normalsize{\textbf{Standard deviation}}\\
The square root of the variance is the standard deviation (std):
\begin{equation}\label{eq:std_def}
    \sigma_x = \sqrt{\mathrm{Var}(x)} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2}.
\end{equation}
% compatible with numpy.std, tested at /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb
The spread in a data set is usually expressed with the standard deviation instead of the variance, as the standard deviation has the same units with the variable $x$.

When the mean of the data set is zero the standard deviation equals the root mean square.

\subsection{More than one variables - Covariance}
In the case that each element of the data set consists of a pair of variables, $\{(x_1, y_1),$ $(x_2, y_2), (x_2, y_2), ...(x_N, y_N)\}$ the covarinace expresses the extend to which $x$ and $y$ tend to vary together. The covariance between $x$ and $y$ is defined as:

\begin{equation}\label{eq:cov_def}
    \mathrm{Cov}(x, y) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle) (y_i-\langle y \rangle).
\end{equation}

It can be seen that the covariance of variable $x$ with itslef equals the variance. In particular, it is written:

\begin{equation}\label{eq:std_var_cov_relationship}
    \mathrm{Cov}(x, x) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2} = \mathrm{Var}(x) = \sigma_x^2.
\end{equation}


\section{Least squares fitting}\label{app:non_linear_fitting}
In sciences, many quantities can not be measured directly but can be inferred from measured data by fitting a model function to them. Common model functions are the Gaussian, polynomial, or sinusoidal. The fitting procedure followed in this thesis is called "least squares" and is described below based on Ref.~\cite{least_square_minimisation}.

Suppose that we have $N$ data points ($x_{i}, y_{i}$) and that $y=f(x,\alpha, \beta)$ is the model function that describes the relationship between the points. The objective of the fit is to determine the optimal parameters $\alpha, \beta$ such as the model function describes best the data points. This is done by minimising the $\chi^2$ statistics with respect to $\alpha$ and $\beta$:
\begin{equation}\label{eq:chi_square}
    \chi^2 = \sum_{i=1}^{N}[y_{i}-f(x_{i},\alpha, \beta)]^2,
\end{equation}

where $y_{i}$ is the observed value and $f(x_{i},\alpha, \beta)$ the expected value from the model. In other words, $\chi^2$ is a measure of deviation between the measurement and the expected result, and thus its minimisation results in the best fit i.e.\ to the optimal parameters $\alpha, \beta$.


\normalsize{\textbf{Error of the fit}}\\
The standard deviation of the fit results, $\sigma \alpha, \sigma \beta$ is estimated by the square root of the diagonal of their covariant matrix:

\begin{equation}\label{eq:cov_matrix_fit_results}
    \begin{pmatrix}
        \sigma_{\alpha}^2 & \mathrm{Cov(\alpha, \beta)}\\
        \mathrm{Cov(\beta, \alpha)} & \sigma_{\beta}^2
        \end{pmatrix}
\end{equation}

In this thesis, the uncertainties of the fit results, $\Delta \alpha, \Delta \beta$, are defined as the standard deviation of the corresponding optimal parameters, $\sigma_{\alpha}$ and  $\sigma_{\beta}$ respectively.

The values of the optimal parameters and their covariance matrix are computed in this thesis using the $\mathrm{scipy.curve \_ fit}$~\cite{scipy_curve_fit} function of the Python programming language.


\section{Detuning with amplitude}\label{app:detuning_with_amplitude}
- The linear detuning is given by the folloiwng formula, for octupole components
- write for detuning coefficients in the numerocultere
- check sondre's thesis

The detuning with amplitude is computed by:

\begin{equation}\label{eq:x_amplDetuning}
    \Delta Q_x = 2(\alpha_{xx}J_x + \alpha_{xy}J_y)
\end{equation}

\begin{equation}\label{eq:y_amplDetuning}
    \Delta Q_y = 2(\alpha_{yy}J_y + \alpha_{yx}J_x)
\end{equation}
where $\alpha_{yy}$, $\alpha_{xx}$ and $\alpha_{xy}=\alpha_{yx}$ are the detuning coefficients with units 1/m.


\normalsize{\textbf{Rms detuning with amplitude}}\\

From the definition of variance, the variance of the vertical amplitude detuning is given by:

\begin{equation}\label{eq:var_amplDetuning}
    \begin{split}
        \mathrm{Var}(\Delta Q_y) = &~\langle \Delta Q_y ^2 \rangle - \langle \Delta Q_y \rangle ^2 \\
        = &~\langle 2^2 (\alpha_{yy} J_y + \alpha_{yx}J_x)^2 \rangle - \langle 2(\alpha_{yy}J_y + \alpha_{yx}J_x) \rangle^2 \\
        = &~2^2 \left [ \langle (\alpha_{yy} J_y + \alpha_{yx}J_x)^2 \rangle - \langle \alpha_{yy}J_y + \alpha_{yx}J_x \rangle^2 \right ] \\
         = &~2^2 \left [  \langle (\alpha_{yy} J_y)^2 + 2\alpha_{yy}\alpha_{yx}J_yJ_x + (\alpha_{yx} J_x)^2  \rangle - (\langle \alpha_{yy}J_y \rangle + \langle \alpha_{yx}J_x \rangle )^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 \langle J_y^2 \rangle + 2\alpha_{yy}\alpha_{yx} \langle J_yJ_x \rangle + \alpha_{yx}^2 \langle J_x^2 \rangle -\alpha_{yy}^2  \langle J_y \rangle^2 - 2\alpha_{yy} \alpha_{yx} \langle J_y \rangle \langle J_x \rangle - \alpha_{yx} ^2 \langle J_x \rangle^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 \langle J_y^2 \rangle + \cancel{2\alpha_{yy}\alpha_{yx} \langle J_yJ_x \rangle} + \alpha_{yx}^2 \langle J_x^2 \rangle -\alpha_{yy}^2  \langle J_y \rangle^2 - \cancel{2\alpha_{yy} \alpha_{yx} \langle J_y J_x \rangle} - \alpha_{yx} ^2 \langle J_x \rangle^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2(\langle J_y^2\rangle - \langle J_y \rangle^2 ) + \alpha_{yx}^2(\langle J_x^2\rangle - \langle J_x \rangle^2 ) \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 Var(J_y) + \alpha_{yx}^2 Var(J_x) \right ]
    \end{split}
\end{equation}

By the definition of root mean square (rms), for the vertical amplitude detuning is written:

\begin{equation}\label{rms_amplitude_detuning}
    \begin{split}
    \Delta Q^{rms}_y = &~\sqrt{\mathrm{Var}(\Delta Q_y)}=\sqrt{2^2\left [ \alpha_{yy}^2 \mathrm{Var}(J_y) + \alpha_{yx}^2 \mathrm{Var}(J_x) \right ]}\\
    =&~2 \sqrt{\alpha_{yy}^2 (J^{rms}_y)^2 + \alpha_{yx}^2 (J^{rms}_x)^2 }\\
    =&~2 \sqrt{\left [\alpha_{yy} (J^{rms}_y)\right ]^2 + \left [ \alpha_{yx} (J^{rms}_x) \right ]^2 }
    \end{split}
\end{equation}

\textbf{Note 1:} In Eq.~\ref{eq:var_amplDetuning} the following properties are used:

The mean of the sum of two variables $X$ and $Y$ is equal to the sum of their means, ie:
\begin{equation}\label{eq:mean_of_sum_property}
    \langle X+Y \rangle = \langle X \rangle + \langle Y \rangle
\end{equation}

If $X$ and $Y$ are independent the mean of their product equals:
\begin{equation}\label{eq:mean_of_product_property}
    \langle X \cdot Y \rangle = \langle X \rangle \cdot  \langle Y \rangle
\end{equation}

It should be mentioned that only the initial actions $J_x$ and $J_y$ are actually independent. The actions later in time, even though they are considered independent in the analysis, are coupled due the non-linearities of the lattice.

\textbf{Note 2:}
\begin{itemize}
    \item $\langle X \rangle$ is consistent with the numpy.mean(X)~\cite{numpy_mean} function of the Python programming language.
    \item $X^{rms}$ is consistent with the numpy.std(X)~\cite{numpy_std} function of the Python programming language. 
\end{itemize}



\normalsize{\textbf{Rms detuning with amplitude from the SPS multiple errors}}\\