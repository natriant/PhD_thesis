\section{Basic terminology}\label{app:statistics_definitions}
This appendix, introduces the basic terminology of statistical analysis and gives the definitions that are used in this thesis. The definitions follow the book by R. J. Barlow~\cite{lvp.b313005720130101} where one can find a more detailed insight.

\subsection{Averages}
\normalsize{\textbf{Arithmetic mean}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the arithmetic mean or just mean of the value of $x$ is:
\begin{equation}\label{eq:mean_def}
    \langle x \rangle = \frac{1}{N} \sum_{i=1}^{N} x_i.
\end{equation}
% compatible with numpy.mean --> /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb

Below, two properties of the arithmetic mean are discussed as they are used in this thesis.
\begin{itemize}
    \item The mean of the sum of two variables $x$ and $y$ is equal to the sum of their means, ie:
    \begin{equation}\label{eq:mean_of_sum_property}
        \langle x+y \rangle = \langle x \rangle + \langle y \rangle
    \end{equation}
    \item If $x$ and $y$ are independent the mean of their product equals:
    \begin{equation}\label{eq:mean_of_product_property}
        \langle x \cdot y \rangle = \langle x \rangle \cdot  \langle y \rangle
    \end{equation}
\end{itemize}


\normalsize{\textbf{Root mean square}}\\
In the classical definition in mathematics, the root mean square (rms) is an alternative to the arithmetic mean and is defined as:
\begin{equation}\label{eq:rms_def}
    x^{rms} = \sqrt{\frac{x_1^2+x_2^2+x_3^2+...+x_N^2}{N}}=\langle x^2 \rangle.
\end{equation}
%In other words it is the square root of the arithmetic mean of the squares of the data set values. It is common in physics and in sciences in general to used the rms value instead of the arithmetic mean when it comes to distributions or when the random variables have both positive and negative values.
\begin{itemize}
\item \textbf{Disclaimer:} It is common in physics and in sciences in general for the term rms to correspond to what is actually defined as standard deviation (see definition in Appendix~\ref{app:meas_spread}). This convention, is also followed in this thesis.
\end{itemize}
\subsection{Measuring the spread}\label{app:meas_spread}
\normalsize{\textbf{Variance}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the variance of $x$  expresses how much it can vary from the mean value, $\langle x \rangle$. The variance, $\mathrm{Var}(x)$, is defined as:
\begin{equation}\label{eq:var_def_1}
    \mathrm{Var}(x) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2.
\end{equation}
% compatible with numpy.var, tested at: /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb
Alternatively, the variance can be expressed in a simpler way as follows (see Ref.~\cite{lvp.b313005720130101} p.24-25):
\begin{equation}\label{eq:var_def_2}
    \mathrm{Var}(x) = \langle x^2 \rangle - \langle x \rangle^2.
\end{equation}

\normalsize{\textbf{Standard deviation}}\\
The square root of the variance is the standard deviation (std):
\begin{equation}\label{eq:std_def_1}
    \sigma_x = \sqrt{\mathrm{Var}(x)} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2},
\end{equation}
or as follows from Eq.~\eqref{eq:var_def_2}:
\begin{equation}\label{eq:std_def_2}
    \sigma_x = \sqrt{\langle x^2 \rangle - \langle x \rangle^2}.
\end{equation}
% compatible with numpy.std, tested at /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb
The spread in a data set is usually expressed with the standard deviation instead of the variance, as the standard deviation has the same units with the variable $x$.

%When the mean of the data set is zero the standard deviation equals the root mean square.

\normalsize{\textbf{Full width half maximum}}\\
An alternative measure of the spread is the full width half maximum (FWHM).


\subsection{Data sets with more than one variables - Covariance}
In the case that each element of the data set consists of a pair of variables, $\{(x_1, y_1),$ $(x_2, y_2), (x_2, y_2), ...(x_N, y_N)\}$ the covarinace expresses the extend to which $x$ and $y$ tend to vary together. The covariance between $x$ and $y$ is defined as:
\begin{equation}\label{eq:cov_def}
    \mathrm{Cov}(x, y) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle) (y_i-\langle y \rangle).
\end{equation}
It can be seen that the covariance of variable $x$ with itslef equals the variance. In particular, it is written:
\begin{equation}\label{eq:std_var_cov_relationship}
    \mathrm{Cov}(x, x) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2} = \mathrm{Var}(x) = \sigma_x^2.
\end{equation}

\section{Least squares fitting}\label{app:non_linear_fitting}
In sciences, many quantities can not be measured directly but can be inferred from measured data by fitting a model function to them. Common model functions are the Gaussian, polynomial, or sinusoidal. The fitting procedure followed in this thesis is called "least squares" and is described below based on Ref.~\cite{least_square_minimisation}.

Suppose that we have $N$ data points ($x_{i}, y_{i}$) and that $y=f(x,\alpha, \beta)$ is the model function that describes the relationship between the points. The objective of the fit is to determine the optimal parameters $\alpha, \beta$ such as the model function describes best the data points. This is done by minimising the $\chi^2$ statistics with respect to $\alpha$ and $\beta$:
\begin{equation}\label{eq:chi_square}
    \chi^2 = \sum_{i=1}^{N}[y_{i}-f(x_{i},\alpha, \beta)]^2,
\end{equation}
where $y_{i}$ is the observed value and $f(x_{i},\alpha, \beta)$ the expected value from the model. In other words, $\chi^2$ is a measure of deviation between the measurement and the expected result, and thus its minimisation results in the best fit i.e.\ to the optimal parameters $\alpha, \beta$.


\normalsize{\textbf{Error of the fit}}\\
The standard deviation of the fit results, $\sigma_\alpha, \sigma_\beta$, is estimated by the square root of the diagonal of their covariant matrix:
\begin{equation}\label{eq:cov_matrix_fit_results}
    \begin{pmatrix}
        \sigma_{\alpha}^2 & \mathrm{Cov(\alpha, \beta)}\\
        \mathrm{Cov(\beta, \alpha)} & \sigma_{\beta}^2
        \end{pmatrix}
\end{equation}
In this thesis, the uncertainties of the fit results, $\Delta \alpha, \Delta \beta$, are defined as the standard deviation, $\sigma_{\alpha}$ and  $\sigma_{\beta}$, of the corresponding optimal parameters.

The values of the optimal parameters and their covariance matrix are computed in this thesis using the $\mathrm{scipy.curve \_ fit}$~\cite{scipy_curve_fit} function of the Python programming language.

\section{Propagation of uncertainty}
For the uncertainty in the mean:
https://www.physics.upenn.edu/sites/default/files/Managing%20Errors%20and%20Uncertainty.pdf



\section{Detuning with amplitude}\label{app:detuning_with_amplitude}
- The linear detuning is given by the folloiwng formula, for octupole components
- write for detuning coefficients in the numerocultere
- check sondre's thesis

The detuning with amplitude is computed by:
\begin{equation}\label{eq:x_amplDetuning}
    \Delta Q_x = 2(\axx \Jx + \alpha_{xy} \Jy)
\end{equation}
\begin{equation}\label{eq:y_amplDetuning}
    \Delta Q_y = 2(\ayy \Jy + \alpha_{yx} \Jx)
\end{equation}
where $\ayy$, $\axx$ and $\axy$ are the detuning coefficients with units [1/m] and $\Jx$, $\Jy$ the action variables.


\normalsize{\textbf{Rms detuning with amplitude}}\\
From the definition of variance, the variance of the vertical amplitude detuning is given by:
\begin{equation}\label{eq:var_amplDetuning}
    \begin{split}
        \mathrm{Var}(\Delta Q_y) = &~\langle \Delta Q_y ^2 \rangle - \langle \Delta Q_y \rangle ^2 \\
        = &~\langle 2^2 (\alpha_{yy} J_y + \alpha_{yx}J_x)^2 \rangle - \langle 2(\alpha_{yy}J_y + \alpha_{yx}J_x) \rangle^2 \\
        = &~2^2 \left [ \langle (\alpha_{yy} J_y + \alpha_{yx}J_x)^2 \rangle - \langle \alpha_{yy}J_y + \alpha_{yx}J_x \rangle^2 \right ] \\
         = &~2^2 \left [  \langle (\alpha_{yy} J_y)^2 + 2\alpha_{yy}\alpha_{yx}J_yJ_x + (\alpha_{yx} J_x)^2  \rangle - (\langle \alpha_{yy}J_y \rangle + \langle \alpha_{yx}J_x \rangle )^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 \langle J_y^2 \rangle + 2\alpha_{yy}\alpha_{yx} \langle J_yJ_x \rangle + \alpha_{yx}^2 \langle J_x^2 \rangle -\alpha_{yy}^2  \langle J_y \rangle^2 - 2\alpha_{yy} \alpha_{yx} \langle J_y \rangle \langle J_x \rangle - \alpha_{yx} ^2 \langle J_x \rangle^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 \langle J_y^2 \rangle + \cancel{2\alpha_{yy}\alpha_{yx} \langle J_yJ_x \rangle} + \alpha_{yx}^2 \langle J_x^2 \rangle -\alpha_{yy}^2  \langle J_y \rangle^2 - \cancel{2\alpha_{yy} \alpha_{yx} \langle J_y J_x \rangle} - \alpha_{yx} ^2 \langle J_x \rangle^2 \right ]\\
         =&~2^2\left [ \alpha_{yy}^2(\langle J_y^2\rangle - \langle J_y \rangle^2 ) + \alpha_{yx}^2(\langle J_x^2\rangle - \langle J_x \rangle^2 ) \right ]\\
         =&~2^2\left [ \alpha_{yy}^2 Var(J_y) + \alpha_{yx}^2 Var(J_x) \right ]
    \end{split}
\end{equation}

In the development of Eq.~\ref{eq:var_amplDetuning} the properties of the mean discussed in Eq.~\eqref{eq:mean_of_sum_property} and~\eqref{eq:mean_of_product_property} are used.


Now, according to the definitions introduced in Appendix~\ref{app:statistics_definitions}, the root mean square (rms) for the vertical amplitude detuning is written:
\begin{equation}\label{eq:rms_amplitude_detuning_1}
    \begin{split}
    \Dqyrms =&~\sigma_{\Delta Q_y}=\sqrt{\mathrm{Var}(\Delta Q_y)}\\
    =&~\sqrt{2^2\left [ \alpha_{yy}^2 \mathrm{Var}(J_y) + \alpha_{yx}^2 \mathrm{Var}(J_x) \right ]}\\
    =&~2 \sqrt{\alpha_{yy}^2 (\sigma_{J_y})^2 + \alpha_{yx}^2 (\sigma_{J_x})^2 }\\
    =&~2 \sqrt{\left [\alpha_{yy} (\sigma_{J_y})\right ]^2 + \left [ \alpha_{yx} (\sigma_{J_x}) \right ]^2 }
    \end{split}
\end{equation}

where $\sigma_{\Jy}$ and $\sigma_{\Jy}$ stand for the standard deviation of the action variables $\Jy$ and $\Jx$ respectively

The actions, $\Jx$ and $\Jx$ follow an exponential distribution (see Eq.~\eqref{eq:Jy_exp_distr}). It is known that for an exponential distribution the mean equals the standard deviation. Therefore, Eq.~\ref{eq:rms_amplitude_detuning_1} can be written as follows:
\begin{equation}\label{eq:rms_amplitute_detuning_2}
    \Dqyrms = 2 \sqrt{\left [\alpha_{yy} \langle \Jy \rangle \right ]^2 + \left [ \alpha_{yx} \langle \Jx \rangle \right ]^2 }.
\end{equation}

Following Eq.~\eqref{eq:geom_emit_actions}, the rms tune spread from amplitude detuning can be also written as:
\begin{equation}\label{eq:rms_amplitute_detuning_3}
    \Dqyrms = 2 \sqrt{(\alpha_{yy} \emitygeom )^2 + ( \alpha_{yx} \emitxgeom)^2 }.
\end{equation}

Equivalently, the horizontal rms tune spread from amplitude detuning is given by:
\begin{equation}\label{eq:rms_amplitute_detuning_3_x}
    \Dqxrms = 2 \sqrt{(\alpha_{xx} \emitxgeom )^2 + ( \alpha_{yx} \emitygeom)^2 }.
\end{equation}


\textbf{Disclaimer:} In the analysis presented above, the actions $\Jx$ and $\Jy$ refer to the initial distribution, for which they are actually independent. The actions later in time, are coupled due to the non-linear
of the lattice. %The actions later in time, even though they are considered independent in the analysis, are coupled due the non-linearities of the lattice.
%\textbf{Note 2:}
%\begin{itemize}
%    \item $\langle X \rangle$ is consistent with the numpy.mean(X)~\cite{numpy_mean} function of the Python programming language.
%    \item $X^{rms}$ is consistent with the numpy.std(X)~\cite{numpy_std} function of the Python programming language. 
%\end{itemize}


\normalsize{\textbf{Rms betatron tune spread from the SPS multiple errors at 270\,GeV}}\\
Here the rms betatron tune spread from the SPS multipole components in the SPS dipole magnets is calculated for beam energy of 270\,GeV. Using the values of the multipoles listed in Table~\ref{tab:sps_mult_270GeV} the corresponding detuning coefficients are obtained with MAD-X. In particular, $\axx$ = 148.39\,1/m, $\axy$ = $-$402.91\,1/m, $\ayy$ = $-$48.7\,1/m. It should be noted that these values are obtained for zero linear chromaticity in both transverse planes.

The tune spread here is computed for the requested initial emittances for the emittance growth measurements of 2018 and 2021, $\emitx = \emity$ = 2\,$\mathrm{\mu m}$. Using Eq.~\eqref{eq:emit_geom_norm_relation} it can be seen that these values corresponds to geometric emittances of $\emitxgeom=\emitygeom$ = 6.95\,nm.
By inserting these values of detuning coefficients and geometric emittances in Eq.. Eq.. the rms tune spread is found to be:



in horizontal and vertical plane respectvily.

% compute detuning coefficients: https://github.com/natriant/exploring_SPS/blob/master/madx_studies/match_octupoles_new_SPS_seq_after_LS2/job001_cmpt_ayy_axx_for_klod_klof.py
% compute rms tune spread: /eos/user/n/natriant/Project_thesis/material/Ch5: Experimental studies 2018/cmpt_DQxy_rms_nominal_SPS_270GeV.ipynb