\thispagestyle{simple} % formatting the first page of each chapter
\section{Basic terminology}\label{app:statistics_definitions}
This appendix, introduces the basic terminology of statistical analysis and gives the definitions that are used in this thesis. The definitions follow the book by R. J. Barlow~\cite{lvp.b313005720130101} where one can find a more detailed insight. Note that the use of the parameters $x$ and $y$ in this Appendix is not related to the co-ordinates introduced to describe the motion of the particles in the transverse plane.

\subsection{Averages}
\normalsize{\textbf{Arithmetic mean}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the arithmetic mean or just mean of the value of $x$ is:
\begin{equation}\label{eq:mean_def}
    \langle x \rangle = \frac{1}{N} \sum_{i=1}^{N} x_i.
\end{equation}
% compatible with numpy.mean --> /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb

Below, two properties of the arithmetic mean are shown since they are used in this thesis.
\begin{itemize}
    \item The mean of the sum of two variables $x$ and $y$ is equal to the sum of their means, ie:
    \begin{equation}\label{eq:mean_of_sum_property}
        \langle x+y \rangle = \langle x \rangle + \langle y \rangle
    \end{equation}
    \item If $x$ and $y$ are independent the mean of their product equals:
    \begin{equation}\label{eq:mean_of_product_property}
        \langle x \cdot y \rangle = \langle x \rangle \cdot  \langle y \rangle
    \end{equation}
\end{itemize}

Another notation for the arithmetic mean that is often found in bibliography is, $\bar{x}$. 


\normalsize{\textbf{Root mean square}}\\
In the classical definition in mathematics, the root mean square (rms) is an alternative to the arithmetic mean and is defined as:
\begin{equation}\label{eq:rms_def}
    x^\mathrm{rms} = \sqrt{\frac{x_1^2+x_2^2+x_3^2+...+x_N^2}{N}}=\sqrt{\langle x^2 \rangle}.
\end{equation}
%In other words it is the square root of the arithmetic mean of the squares of the data set values. It is common in physics and in sciences in general to used the rms value instead of the arithmetic mean when it comes to distributions or when the random variables have both positive and negative values.
%\begin{itemize}
%\item \textbf{Disclaimer:} It is common in physics and in sciences in general for the term rms to correspond to what is actually defined as standard deviation (see definition in Appendix~\ref{app:meas_spread}). This convention, is also followed in this thesis.
%\end{itemize}

Often in physics and engineering, the term rms is used as a synonym for the standard deviation when one refers to the rms from a given baseline~\cite{rms_std}.
% source: https://mathworld.wolfram.com/Root-Mean-Square.html



\subsection{Measuring the spread}\label{app:meas_spread}
\normalsize{\textbf{Variance}}\\
For a data set of $N$ data $\{ x_1, x_2, x_3, ..., x_N \}$ the variance of $x$  expresses how much it can vary from the mean value, $\langle x \rangle$. The variance, $\mathrm{Var}(x)$, is defined as:
\begin{equation}\label{eq:var_def_1}
    \mathrm{Var}(x) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2.
\end{equation}
% compatible with numpy.var, tested at: /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb
Alternatively, the variance can be expressed in a simpler way as follows (see p.24-25 in~\cite{lvp.b313005720130101}):
\begin{equation}\label{eq:var_def_2}
    \mathrm{Var}(x) = \langle x^2 \rangle - \langle x \rangle^2.
\end{equation}

\normalsize{\textbf{Standard deviation}}\\
The square root of the variance is the standard deviation (std):
\begin{equation}\label{eq:std_def_1}
    \sigma_x = \sqrt{\mathrm{Var}(x)} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2},
\end{equation}
or as follows from Eq.~\eqref{eq:var_def_2}:
\begin{equation}\label{eq:std_def_2}
    \sigma_x = \sqrt{\langle x^2 \rangle - \langle x \rangle^2}.
\end{equation}
% compatible with numpy.std, tested at /eos/user/n/natriant/Project_thesis/material/Appendix/test_numpy_definitions.ipynb
The spread in a data set is usually expressed with the standard deviation instead of the variance, as the standard deviation has the same units as the variable $x$.

%When the mean of the data set is zero the standard deviation equals the root mean square.

%\normalsize{\textbf{Full width half maximum}}\\
%An alternative measure of the spread is the full width half maximum (FWHM).


\subsection{Data sets with more than one variables - Covariance}
In the case that each element of the data set consists of a pair of variables, $\{(x_1, y_1),$ $(x_2, y_2), (x_2, y_2), ...(x_N, y_N)\}$ the covariance expresses the extend to which $x$ and $y$ tend to vary together. The covariance between $x$ and $y$ is defined as:
\begin{equation}\label{eq:cov_def}
    \mathrm{Cov}(x, y) = \frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle) (y_i-\langle y \rangle).
\end{equation}
It can be seen that the covariance of variable $x$ with itslef equals the variance. In particular, it is written:
\begin{equation}\label{eq:std_var_cov_relationship}
    \mathrm{Cov}(x, x) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i-\langle x \rangle)^2} = \mathrm{Var}(x) = \sigma_x^2.
\end{equation}

\textbf{Covariance matrix}\\
The covariance as defined above is only calculated between two variables. To express the covariance values of each pair of variables, the covariance matrix or Sigma matrix is introduced as follows and is:

\begin{equation}\label{eq:Sigma_matrix}
    \Sigma = \begin{pmatrix}
        \mathrm{Cov}(x,x) & \mathrm{Cov}(x,y) \\ 
        \mathrm{Cov}(y,x) & \mathrm{Cov}(y,y) 
        \end{pmatrix}  = \begin{pmatrix}
            \sigma_x^2 & \mathrm{Cov}(x,y) \\ 
            \mathrm{Cov}(y,x) & \sigma_y^2 
            \end{pmatrix},
\end{equation}

since the covariance between the same variables equals to the variance (Eq.~\eqref{eq:std_var_cov_relationship}).

If the data set is a distribution the covariance matrix is a parameter of the distribution.


\section{Least squares fitting}\label{app:non_linear_fitting}
In sciences, many quantities can not be measured directly but can be inferred from measured data by fitting a model function to them. Common model functions are the Gaussian, polynomial, or sinusoidal. The fitting procedure followed in this thesis is called "least squares" and is described below, based on~\cite{least_square_minimisation}.

Suppose that we have $N$ data points ($x_{i}, y_{i}$) and that $y=f(x,\alpha, \beta)$ is the model function that describes the relationship between the points. The objective of the fit is to determine the optimal parameters $\alpha, \beta$ such as the model function describes best the data points. This is done by minimising the $\chi^2$ statistics with respect to $\alpha$ and $\beta$:
\begin{equation}\label{eq:chi_square}
    \chi^2 = \sum_{i=1}^{N}[y_{i}-f(x_{i},\alpha, \beta)]^2,
\end{equation}
where $y_{i}$ is the observed value and $f(x_{i},\alpha, \beta)$ the expected value from the model. In other words, $\chi^2$ is a measure of deviation between the measurement and the expected result, and thus its minimisation results in the best fit i.e.~to the optimal parameters $\alpha, \beta$.

\normalsize{\textbf{Weighted least squares fitting}}\\
Suppose that we have $N$ data points ($x_i, y_i \pm \Delta y_i $), where $ \Delta y_i$ is the uncertaintyof $y_i$ y, and that $y=f(x,\alpha, \beta)$ is the model function that describes the relationship between the points. To define the optimal parameters $\alpha, \beta$ taking into account the imapct of the uncertainty $\Delta y_i$, Eq.~\eqref{eq:chi_square} is written as:
\begin{equation}\label{eq:chi_square_weights}
    \chi^2 = \sum_{i=1}^{N}\frac{[y_{i}-f(x_{i},\alpha, \beta)]^2}{\Delta y_i^2}
\end{equation}

\normalsize{\textbf{Uncertainty of the fit}}\\
The standard deviation of the fit results, $\sigma_\alpha, \sigma_\beta$, is estimated by the square root of the diagonal of their covariant matrix:
\begin{equation}\label{eq:cov_matrix_fit_results}
    \begin{pmatrix}
        \sigma_{\alpha}^2 & \mathrm{Cov(\alpha, \beta)}\\
        \mathrm{Cov(\beta, \alpha)} & \sigma_{\beta}^2
        \end{pmatrix}
\end{equation}
In this thesis, the uncertainties of the fit results, $\Delta \alpha, \Delta \beta$, are defined as the standard deviation, $\sigma_{\alpha}$ and  $\sigma_{\beta}$, of the corresponding optimal parameters.

The values of the optimal parameters and their covariance matrix are computed in this thesis using the $\mathrm{scipy.curve \_ fit}$~\cite{scipy_curve_fit} function of the Python programming language.

\section{Propagation of uncertainty}\label{app:uncertainty_propagation}
Suppose that $y$ is related to $N$ independent variables $\{x_1, x_2, ..., x_N\}$ with the following function:
\begin{equation}
    y=f(x_1, x_2, .. x_N).
\end{equation}
If $\{\Delta x_1, \Delta x_2, ..., \Delta x_N\}$ the uncertainties of $\{x_1, x_2, ..., x_N\}$ respectively, the uncertainty of $y$, is given by~\cite{lvp.b313005720130101}:
\begin{equation}\label{eq:uncertainty_propagation}
    \Delta y = \sqrt{\left ( \frac{\partial f}{\partial x_1} \Delta x_1  \right )^2 + \left ( \frac{\partial f}{\partial x_2} \Delta x_2  \right )^2 + \dots + \left ( \frac{\partial f}{\partial x_N} \Delta x_N  \right )^2}
\end{equation}


%For the uncertainty in the mean:
%https://www.physics.upenn.edu/sites/default/files/Managing%20Errors%20and%20Uncertainty.pdf


